 
 
# Use seaborn for pairplot
!pip install -q seaborn
 
# Use some functions from tensorflow_docs
!pip install -q git+https://github.com/tensorflow/docs
 
from __future__ import absolute_import, division, print_function, unicode_literals
 
import pathlib
 
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
 
import tensorflow as tf
 
from tensorflow import keras
from tensorflow.keras import layers
 
print(tf.__version__)
 
 
import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling
dataset_path = keras.utils.get_file("auto-mpg.data", "https://drive.google.com/uc?authuser=0&id=1WM0TWAAtJ7-LwzbgBJ-XIm2V20F8gWON&export=download")
dataset_path
 
column_names = ['Cylinders', 'MPG','Displacement','Horsepower','Weight',
                'Acceleration', 'Model Year', 'Origin']
raw_dataset = pd.read_csv(dataset_path, names=column_names,
                      na_values = "?", comment='\t',
                      sep=" ", skipinitialspace=True)
 
dataset = raw_dataset.copy()
dataset.tail()
 
 
 
dataset.isna().sum()
 
 
 
dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')
dataset.tail()
 
 
 
train_dataset = dataset.sample(frac=0.8,random_state=0)
test_dataset = dataset.drop(train_dataset.index)
 
 
 
 
 
train_stats = train_dataset.describe()
train_stats.pop("MPG")
train_stats = train_stats.transpose()
train_stats
 
train_labels = train_dataset.pop('MPG')
test_labels = test_dataset.pop('MPG')
 
 
def norm(x):
  return (x - train_stats['mean']) / train_stats['std']
normed_train_data = norm(train_dataset)
normed_test_data = norm(test_dataset)
print(train_stats['std'])
 
 
def build_model():
  model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
  ])
 
  optimizer = tf.keras.optimizers.RMSprop(0.001)
 
  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae', 'mse'])
  return model
 
model = build_model()
 
 
 
model.summary()
 
 
 
example_batch = normed_train_data[:10]
example_result = model.predict(example_batch)
example_result
 
 
 
 
EPOCHS = 1000
 
history = model.fit(
  normed_train_data, train_labels,
  epochs=EPOCHS, validation_split = 0.2, verbose=0,
  callbacks=[tfdocs.modeling.EpochDots()])
 
 
hist = pd.DataFrame(history.history)
hist['epoch'] = history.epoch
hist.tail()
 
 
 
 
model = build_model()
 
# The patience parameter is the amount of epochs to check for improvement
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
 
early_history = model.fit(normed_train_data, train_labels, 
                    epochs=EPOCHS, validation_split = 0.2, verbose=0, 
                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])
 
 
 
loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)
 
print("Testing set Mean Abs Error: {:5.2f} MPG".format(mae))
 
 
test_predictions = model.predict(normed_test_data).flatten()
 
a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True Values [MPG]')
plt.ylabel('Predictions [MPG]')
lims = [0, 50]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)
 
 
error = test_predictions - test_labels
plt.hist(error, bins = 25)
plt.xlabel("Prediction Error [MPG]")
_ = plt.ylabel("Count")
